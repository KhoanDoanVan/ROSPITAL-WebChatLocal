import ollama
from langchain_community.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
import os

# ğŸ”¥ Load láº¡i FAISS database
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")




# Sá»­ dá»¥ng Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i
faiss_path = os.path.abspath("faiss_index")

# âœ… Debug: Kiá»ƒm tra xem thÆ° má»¥c FAISS cÃ³ tá»“n táº¡i khÃ´ng
if not os.path.exists(faiss_path):
    raise FileNotFoundError(f"âŒ FAISS index not found at {faiss_path}")

# Load FAISS vá»›i quyá»n Ä‘á»c an toÃ n
print(f"ğŸ“Œ Loading FAISS index from: {faiss_path}")
vector_db = FAISS.load_local(faiss_path, embeddings=embedding_model, allow_dangerous_deserialization=True)

# âœ… Debug: Kiá»ƒm tra sá»‘ lÆ°á»£ng tÃ i liá»‡u trong FAISS
if hasattr(vector_db, "index") and vector_db.index.ntotal == 0:
    raise ValueError("âŒ FAISS index is empty! No data found.")

print(f"âœ… FAISS index loaded successfully! Total vectors: {vector_db.index.ntotal}")




# ğŸ” Nháº­p cÃ¢u há»i tá»« ngÆ°á»i dÃ¹ng
query = input("Nháº­p cÃ¢u há»i cá»§a báº¡n: ")

# ğŸ“Œ Truy váº¥n FAISS Ä‘á»ƒ láº¥y tÃ i liá»‡u liÃªn quan
retrieved_docs = vector_db.similarity_search(query, k=5)  # Láº¥y 5 Ä‘oáº¡n phÃ¹ há»£p nháº¥t
retrieved_text = "\n\n".join([doc.page_content for doc in retrieved_docs])

# ğŸ§  Format prompt cho LLaMA 3.1
prompt = f"""
You are an AI assistant that answers questions based on retrieved context.

Context:
{retrieved_text}

Question:
{query}

Answer:
"""

# ğŸ’¡ Gá»­i prompt Ä‘áº¿n LLaMA 3.1
response = ollama.chat(model="llama3.1", messages=[{"role": "user", "content": prompt}])


# ğŸ“Œ Hiá»ƒn thá»‹ káº¿t quáº£
print("\nğŸ“Œ Answer:", response["message"]["content"])